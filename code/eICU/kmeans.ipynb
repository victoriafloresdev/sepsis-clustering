{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 33)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/grad/ccomp/23/haniel.botelho/Documents/git_ssh/sepsis-clustering/code/kmeans.ipynb Cell 1\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/grad/ccomp/23/haniel.botelho/Documents/git_ssh/sepsis-clustering/code/kmeans.ipynb#W0sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Aplicar o StandardScaler nas colunas numéricas\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/grad/ccomp/23/haniel.botelho/Documents/git_ssh/sepsis-clustering/code/kmeans.ipynb#W0sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m scaler \u001b[39m=\u001b[39m StandardScaler()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/grad/ccomp/23/haniel.botelho/Documents/git_ssh/sepsis-clustering/code/kmeans.ipynb#W0sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m data[numeric_columns] \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mfit_transform(data[numeric_columns])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/grad/ccomp/23/haniel.botelho/Documents/git_ssh/sepsis-clustering/code/kmeans.ipynb#W0sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Aplicar o LabelEncoder nas colunas categóricas\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/grad/ccomp/23/haniel.botelho/Documents/git_ssh/sepsis-clustering/code/kmeans.ipynb#W0sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m label_encoder \u001b[39m=\u001b[39m LabelEncoder()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py:295\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    294\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 295\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    296\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    297\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    298\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    299\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    300\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    301\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis object (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m) has a `transform`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[39mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:876\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 876\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py:912\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \n\u001b[1;32m    882\u001b[0m \u001b[39mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[39m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    911\u001b[0m first_call \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mn_samples_seen_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    913\u001b[0m     X,\n\u001b[1;32m    914\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    915\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[1;32m    916\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    917\u001b[0m     reset\u001b[39m=\u001b[39;49mfirst_call,\n\u001b[1;32m    918\u001b[0m )\n\u001b[1;32m    919\u001b[0m n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m    921\u001b[0m \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[39m=\u001b[39m _check_y(y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1072\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1071\u001b[0m     \u001b[39mif\u001b[39;00m n_samples \u001b[39m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1072\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1073\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m sample(s) (shape=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) while a\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m minimum of \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m is required\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1075\u001b[0m             \u001b[39m%\u001b[39m (n_samples, array\u001b[39m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1076\u001b[0m         )\n\u001b[1;32m   1078\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_features \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m array\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m   1079\u001b[0m     n_features \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 33)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Carregar dados do CSV\n",
    "data = pd.read_csv('exemplo.csv')\n",
    "data = data.dropna()\n",
    "# Pré-processamento, se necessário (por exemplo, normalização)\n",
    "\n",
    "# Lista das colunas a serem normalizadas com o StandardScaler (colunas 1 a 4)\n",
    "numeric_columns = ['sofascore', 'WBC',\n",
    "       'WBCurine', 'CRP', 'lactate_x', 'celsius', 'ESR', 'bands',\n",
    "       'physicalexamoffset_x',   'PAO2',\n",
    "       'FIO2', 'bicarbonate', 'heartRate', 'lactate_y', 'systolic', 'troponin',\n",
    "       'bloodUreaNitrogen', 'creatinine', 'alanine', 'aspartate', 'bilirubin',\n",
    "       'hemoglobin', 'iqr', 'platelets', 'Albumin', 'Chloride', 'Clucose',\n",
    "       'Sodium', 'physicalexamoffset_y', 'motor', 'eyes', 'verbal',\n",
    "       'gcs total']\n",
    "\n",
    "# Lista das colunas a serem codificadas com o LabelEncoder (colunas 5, 7, 8, 9 e 10)\n",
    "categorical_columns = ['gender', 'physicalexamvalue','age','physicalexamtext','ethnicity', 'apacheadmissiondx']\n",
    "\n",
    "# Aplicar o StandardScaler nas colunas numéricas\n",
    "scaler = StandardScaler()\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Aplicar o LabelEncoder nas colunas categóricas\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_columns:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Escolher o número de clusters (K) usando o método Lloyd\n",
    "silhouette = []\n",
    "inertia = []\n",
    "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette.append(silhouette_score(data, kmeans.predict(data)))\n",
    "\n",
    "# Plotar o gráfico do método Lloyd\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Number of clusters')\n",
    "ax1.set_ylabel('Inertia', color=color)\n",
    "ax1.plot(range(2, 17), inertia, marker='o', color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)    \n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Silhouette Score', color=color)  \n",
    "ax2.plot(range(2, 17), silhouette, marker='x', color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.title('Inertia and Silhouette Score Comparison')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Com base no gráfico, escolha o número ideal de clusters e execute K-Means\n",
    "k = 4  # Por exemplo, digamos que o número ideal de clusters seja 3\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "clusters = kmeans.fit_predict(data_transformed)\n",
    "\n",
    "# Adicione os rótulos de cluster de volta ao DataFrame original\n",
    "data['Cluster'] = clusters\n",
    "\n",
    "# Se desejar, você pode salvar os resultados em um novo arquivo CSV\n",
    "data.to_csv('resultados.csv', index=False)\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=3)\n",
    "data_pca = pca.fit_transform(data_transformed)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Extrair os componentes principais (PC1, PC2, PC3) dos dados reduzidos\n",
    "PC1 = data_pca[:, 0]\n",
    "PC2 = data_pca[:, 1]\n",
    "PC3 = data_pca[:, 2]\n",
    "\n",
    "# Plotar os dados nos eixos principais\n",
    "ax.scatter(PC1, PC2, PC3)\n",
    "\n",
    "# Configurar os rótulos dos eixos\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "\n",
    "plt.title('PCA Plot')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
